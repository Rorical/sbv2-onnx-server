version: '3.8'

services:
  sbv2-onnx-server:
    # This service builds the CUDA-enabled Docker image.
    # It requires an NVIDIA GPU and the NVIDIA Container Toolkit.
    build:
      context: .
      target: cuda
      args:
        CARGO_FEATURES: cuda
    ports:
      - "8080:8080"
    volumes:
      # Mount your local model directories into the container.
      - ./models:/app/models
      - ./bert:/app/bert
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    command:
      - "./sbv2_onnx_server"
      - "--model"
      - "/app/models/model.onnx"
      - "--config"
      - "/app/models/config.json"
      - "--style-vectors"
      - "/app/models/style_vectors.npy"
      - "--bert-root"
      - "/app/bert/"
      - "--listen"
      - "0.0.0.0:8080"
